{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.\n",
    "a) Write a Python script to create a panda series: one for the true classes and\n",
    "one for the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    2\n",
       "5    0\n",
       "6    2\n",
       "7    1\n",
       "8    0\n",
       "9    1\n",
       "Name: True Classes, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given data\n",
    "true_classes = [1, 0, 1, 1, 2, 0, 2, 1, 0, 1]\n",
    "predicted_classes = [1, 0, 0, 1, 2, 0, 1, 2, 0, 1]\n",
    "\n",
    "# Creating pandas Series\n",
    "true_series = pd.Series(true_classes, name='True Classes')\n",
    "predicted_series = pd.Series(predicted_classes, name='Predicted Classes')\n",
    "\n",
    "true_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    2\n",
       "5    0\n",
       "6    1\n",
       "7    2\n",
       "8    0\n",
       "9    1\n",
       "Name: Predicted Classes, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the accuracy_score function from scikit-learn to calculate the\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(true_series, predicted_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Using the same dataset as problem 1 compute the accuracy from first principles, i.e., compare the lists and calculate the accuracy with the accuracy formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Manually calculating accuracy\n",
    "correct_predictions = sum([1 for true, pred in zip(true_classes, predicted_classes) if true == pred])\n",
    "accuracy_manual = correct_predictions / len(true_classes)\n",
    "print(f\"Manual Accuracy: {accuracy_manual:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "a) Write a Python script correctly calculating the precision by using precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Cancer dataset\n",
    "cancer_true = [1, 0, 1, 1, 0, 0, 1, 0, 1, 0]\n",
    "cancer_predicted = [1, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "\n",
    "# Calculating precision\n",
    "precision = precision_score(cancer_true, cancer_predicted)\n",
    "print(f\"Precision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Write a python script using the precision formula calculating the precision for the cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Calculating true positives and false positives\n",
    "tp = sum([1 for true, pred in zip(cancer_true, cancer_predicted) if true == 1 and pred == 1])\n",
    "fp = sum([1 for true, pred in zip(cancer_true, cancer_predicted) if true == 0 and pred == 1])\n",
    "\n",
    "# Calculating precision\n",
    "precision_manual = tp / (tp + fp)\n",
    "print(f\"Manual Precision: {precision_manual:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Explain why precision would be prioritized over accuracy in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cancer detection, precision is prioritized because it measures the proportion of true positive predictions out of all positive predictions. It minimizes false positives, which is important in cancer detection to avoid incorrectly diagnosing healthy individuals as having cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Using the toy dataset in question 3 calculate the recall by using the recall formula and by using the sklearn function recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.60\n",
      "Manual Recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Calculating recall using sklearn\n",
    "recall = recall_score(cancer_true, cancer_predicted)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Manually calculating recall\n",
    "fn = sum([1 for true, pred in zip(cancer_true, cancer_predicted) if true == 1 and pred == 0])\n",
    "recall_manual = tp / (tp + fn)\n",
    "print(f\"Manual Recall: {recall_manual:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.\n",
    "a) Discuss the implications of these metrics in problems 3 and 4 in the context of cancer detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision aims to reduce false positives, which is crucial when a wrong positive diagnosis could lead to unnecessary stress, further invasive tests, and treatment.\n",
    "- Recall emphasizes reducing false negatives, ensuring that those who actually have cancer are identified. Missing a cancer diagnosis (false negative) could result in missed treatment opportunities and worse health outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Which metric is more important in this context and why?\n",
    "- In cancer detection, recall is often more critical than precision. This is because, in most cases, a false negative cancer diagnosis has worse consequences than a mistaken positive diagnosis. As early detection and treatment are essential for better patient outcomes, it is imperative that all genuine cancer cases are correctly diagnosed. To guarantee high recall while preventing an excessive number of false positives, precision and recall should be balanced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
